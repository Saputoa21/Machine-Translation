{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1c337fcc0f344653bfcf1184e819e784": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d675460a1f004c7cb75141c3677d243c",
              "IPY_MODEL_879253a5ebc941c787ea1f79297bca39",
              "IPY_MODEL_2bbf3280f99048058d038270cc909c9b"
            ],
            "layout": "IPY_MODEL_729d2cdf2a1c4b4583ac8ee8dc69f545"
          }
        },
        "d675460a1f004c7cb75141c3677d243c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ef9c26fa51174462bb48927b0cec7c77",
            "placeholder": "​",
            "style": "IPY_MODEL_5fad5b2068934a77838bf7df7da49666",
            "value": "Map: 100%"
          }
        },
        "879253a5ebc941c787ea1f79297bca39": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_178e42b7b8e04ad89d59819dc7166df2",
            "max": 5000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_32415da382cf431488c19e373e761f55",
            "value": 5000
          }
        },
        "2bbf3280f99048058d038270cc909c9b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ad42cea1987f4060b59afadb37362c64",
            "placeholder": "​",
            "style": "IPY_MODEL_2561a930ee0a4886a173ce8a92567d19",
            "value": " 5000/5000 [00:01&lt;00:00, 3615.00 examples/s]"
          }
        },
        "729d2cdf2a1c4b4583ac8ee8dc69f545": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ef9c26fa51174462bb48927b0cec7c77": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5fad5b2068934a77838bf7df7da49666": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "178e42b7b8e04ad89d59819dc7166df2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "32415da382cf431488c19e373e761f55": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ad42cea1987f4060b59afadb37362c64": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2561a930ee0a4886a173ce8a92567d19": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7997d8194f51436b8b66cb1ed90991cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2c1a26923c504c7089bb5ae7f7a2db79",
              "IPY_MODEL_181e3bad4b774b9cb34ea064c04568f5",
              "IPY_MODEL_fc01c1c42747444ea9bf1ec41e91e1ff"
            ],
            "layout": "IPY_MODEL_6c86ab33fcaf4f8daf21e4f5e53d2f1f"
          }
        },
        "2c1a26923c504c7089bb5ae7f7a2db79": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f214a7bfd99c415aa74614fcc62b87fd",
            "placeholder": "​",
            "style": "IPY_MODEL_83993188f3ef422dbe04b580a38297e5",
            "value": "Map: 100%"
          }
        },
        "181e3bad4b774b9cb34ea064c04568f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5a5121b5285742cb9d254493fab35233",
            "max": 200,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_dd0da857fa2b4eb6916764e10239642b",
            "value": 200
          }
        },
        "fc01c1c42747444ea9bf1ec41e91e1ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_faaa5163e33743418a0711af12a40951",
            "placeholder": "​",
            "style": "IPY_MODEL_1fb8ebdafbed419c87d4b4a1fe1ab7ba",
            "value": " 200/200 [00:00&lt;00:00, 3078.79 examples/s]"
          }
        },
        "6c86ab33fcaf4f8daf21e4f5e53d2f1f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f214a7bfd99c415aa74614fcc62b87fd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "83993188f3ef422dbe04b580a38297e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5a5121b5285742cb9d254493fab35233": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dd0da857fa2b4eb6916764e10239642b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "faaa5163e33743418a0711af12a40951": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1fb8ebdafbed419c87d4b4a1fe1ab7ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6a87442948f644c894c4502f29c91e96": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1cd94ec6b6034578bbef09e0f7921f6e",
              "IPY_MODEL_c16ce12516544536807017369067e1d3",
              "IPY_MODEL_51455094a8d14868ad28e9a238951074"
            ],
            "layout": "IPY_MODEL_d362804d450a437ebb58c79209ce55d2"
          }
        },
        "1cd94ec6b6034578bbef09e0f7921f6e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4cae0939344642448206d9b1fd8c92fc",
            "placeholder": "​",
            "style": "IPY_MODEL_db964ff65cbb41da8727d10e031cddad",
            "value": "Map: 100%"
          }
        },
        "c16ce12516544536807017369067e1d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_54a991f97a9046a0be675b5edcf1f5db",
            "max": 200,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_291bf5c1d9ca4369b8438d5d945978b9",
            "value": 200
          }
        },
        "51455094a8d14868ad28e9a238951074": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8d0b6f6d8d51413bb7645e154698d8b2",
            "placeholder": "​",
            "style": "IPY_MODEL_c4ac98306d91473cb043f52f7b744a24",
            "value": " 200/200 [00:00&lt;00:00, 3055.59 examples/s]"
          }
        },
        "d362804d450a437ebb58c79209ce55d2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4cae0939344642448206d9b1fd8c92fc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "db964ff65cbb41da8727d10e031cddad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "54a991f97a9046a0be675b5edcf1f5db": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "291bf5c1d9ca4369b8438d5d945978b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8d0b6f6d8d51413bb7645e154698d8b2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c4ac98306d91473cb043f52f7b744a24": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Saputoa21/Machine-Translation/blob/main/finetune_FreezemBART_AdvanceMT2025W.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "IvMMFlaBEsMc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ad0062c-4288-453d-a833-f6ea07346254"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers==4.44.2 (from transformers[sentencepiece]==4.44.2)\n",
            "  Downloading transformers-4.44.2-py3-none-any.whl.metadata (43 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/43.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers==4.44.2->transformers[sentencepiece]==4.44.2) (3.20.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.12/dist-packages (from transformers==4.44.2->transformers[sentencepiece]==4.44.2) (0.36.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers==4.44.2->transformers[sentencepiece]==4.44.2) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers==4.44.2->transformers[sentencepiece]==4.44.2) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers==4.44.2->transformers[sentencepiece]==4.44.2) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers==4.44.2->transformers[sentencepiece]==4.44.2) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers==4.44.2->transformers[sentencepiece]==4.44.2) (2.32.4)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.12/dist-packages (from transformers==4.44.2->transformers[sentencepiece]==4.44.2) (0.6.2)\n",
            "Collecting tokenizers<0.20,>=0.19 (from transformers==4.44.2->transformers[sentencepiece]==4.44.2)\n",
            "  Downloading tokenizers-0.19.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers==4.44.2->transformers[sentencepiece]==4.44.2) (4.67.1)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from transformers[sentencepiece]==4.44.2) (5.29.5)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /usr/local/lib/python3.12/dist-packages (from transformers[sentencepiece]==4.44.2) (0.2.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.44.2->transformers[sentencepiece]==4.44.2) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.44.2->transformers[sentencepiece]==4.44.2) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.44.2->transformers[sentencepiece]==4.44.2) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.44.2->transformers[sentencepiece]==4.44.2) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.44.2->transformers[sentencepiece]==4.44.2) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.44.2->transformers[sentencepiece]==4.44.2) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.44.2->transformers[sentencepiece]==4.44.2) (2025.10.5)\n",
            "Downloading transformers-4.44.2-py3-none-any.whl (9.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m66.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tokenizers-0.19.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m80.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tokenizers, transformers\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.22.1\n",
            "    Uninstalling tokenizers-0.22.1:\n",
            "      Successfully uninstalled tokenizers-0.22.1\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.57.1\n",
            "    Uninstalling transformers-4.57.1:\n",
            "      Successfully uninstalled transformers-4.57.1\n",
            "Successfully installed tokenizers-0.19.1 transformers-4.44.2\n",
            "Collecting datasets==2.19.0\n",
            "  Downloading datasets-2.19.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets==2.19.0) (3.20.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from datasets==2.19.0) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets==2.19.0) (18.1.0)\n",
            "Collecting pyarrow-hotfix (from datasets==2.19.0)\n",
            "  Downloading pyarrow_hotfix-0.7-py3-none-any.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets==2.19.0) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets==2.19.0) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.12/dist-packages (from datasets==2.19.0) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.12/dist-packages (from datasets==2.19.0) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets==2.19.0) (3.6.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.12/dist-packages (from datasets==2.19.0) (0.70.16)\n",
            "Collecting fsspec<=2024.3.1,>=2023.1.0 (from fsspec[http]<=2024.3.1,>=2023.1.0->datasets==2.19.0)\n",
            "  Downloading fsspec-2024.3.1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.12/dist-packages (from datasets==2.19.0) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.2 in /usr/local/lib/python3.12/dist-packages (from datasets==2.19.0) (0.36.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from datasets==2.19.0) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets==2.19.0) (6.0.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets==2.19.0) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets==2.19.0) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets==2.19.0) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets==2.19.0) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets==2.19.0) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets==2.19.0) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets==2.19.0) (1.22.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.21.2->datasets==2.19.0) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.21.2->datasets==2.19.0) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->datasets==2.19.0) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->datasets==2.19.0) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->datasets==2.19.0) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->datasets==2.19.0) (2025.10.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets==2.19.0) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets==2.19.0) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets==2.19.0) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets==2.19.0) (1.17.0)\n",
            "Downloading datasets-2.19.0-py3-none-any.whl (542 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m542.0/542.0 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.3.1-py3-none-any.whl (171 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m172.0/172.0 kB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyarrow_hotfix-0.7-py3-none-any.whl (7.9 kB)\n",
            "Installing collected packages: pyarrow-hotfix, fsspec, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.0\n",
            "    Uninstalling fsspec-2025.3.0:\n",
            "      Successfully uninstalled fsspec-2025.3.0\n",
            "  Attempting uninstall: datasets\n",
            "    Found existing installation: datasets 4.0.0\n",
            "    Uninstalling datasets-4.0.0:\n",
            "      Successfully uninstalled datasets-4.0.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2024.3.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-2.19.0 fsspec-2024.3.1 pyarrow-hotfix-0.7\n",
            "Collecting evaluate==0.4.3\n",
            "  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from evaluate==0.4.3) (2.19.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from evaluate==0.4.3) (2.0.2)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.12/dist-packages (from evaluate==0.4.3) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from evaluate==0.4.3) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.12/dist-packages (from evaluate==0.4.3) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.12/dist-packages (from evaluate==0.4.3) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from evaluate==0.4.3) (3.6.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.12/dist-packages (from evaluate==0.4.3) (0.70.16)\n",
            "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]>=2021.05.0->evaluate==0.4.3) (2024.3.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from evaluate==0.4.3) (0.36.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from evaluate==0.4.3) (25.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets>=2.0.0->evaluate==0.4.3) (3.20.0)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.0.0->evaluate==0.4.3) (18.1.0)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.12/dist-packages (from datasets>=2.0.0->evaluate==0.4.3) (0.7)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.12/dist-packages (from datasets>=2.0.0->evaluate==0.4.3) (3.13.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.0.0->evaluate==0.4.3) (6.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.7.0->evaluate==0.4.3) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.7.0->evaluate==0.4.3) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate==0.4.3) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate==0.4.3) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate==0.4.3) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate==0.4.3) (2025.10.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->evaluate==0.4.3) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->evaluate==0.4.3) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->evaluate==0.4.3) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets>=2.0.0->evaluate==0.4.3) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets>=2.0.0->evaluate==0.4.3) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets>=2.0.0->evaluate==0.4.3) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets>=2.0.0->evaluate==0.4.3) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets>=2.0.0->evaluate==0.4.3) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets>=2.0.0->evaluate==0.4.3) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets>=2.0.0->evaluate==0.4.3) (1.22.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate==0.4.3) (1.17.0)\n",
            "Downloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: evaluate\n",
            "Successfully installed evaluate-0.4.3\n",
            "Collecting accelerate==0.34.2\n",
            "  Downloading accelerate-0.34.2-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.12/dist-packages (from accelerate==0.34.2) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from accelerate==0.34.2) (25.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate==0.34.2) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from accelerate==0.34.2) (6.0.3)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.12/dist-packages (from accelerate==0.34.2) (2.8.0+cu126)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from accelerate==0.34.2) (0.36.0)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from accelerate==0.34.2) (0.6.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.21.0->accelerate==0.34.2) (3.20.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.21.0->accelerate==0.34.2) (2024.3.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.21.0->accelerate==0.34.2) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.21.0->accelerate==0.34.2) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.21.0->accelerate==0.34.2) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.21.0->accelerate==0.34.2) (1.2.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (3.4.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.10.0->accelerate==0.34.2) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.10.0->accelerate==0.34.2) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate==0.34.2) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate==0.34.2) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate==0.34.2) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate==0.34.2) (2025.10.5)\n",
            "Downloading accelerate-0.34.2-py3-none-any.whl (324 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m324.4/324.4 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: accelerate\n",
            "  Attempting uninstall: accelerate\n",
            "    Found existing installation: accelerate 1.11.0\n",
            "    Uninstalling accelerate-1.11.0:\n",
            "      Successfully uninstalled accelerate-1.11.0\n",
            "Successfully installed accelerate-0.34.2\n",
            "Collecting sacrebleu==2.4.3\n",
            "  Downloading sacrebleu-2.4.3-py3-none-any.whl.metadata (51 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting portalocker (from sacrebleu==2.4.3)\n",
            "  Downloading portalocker-3.2.0-py3-none-any.whl.metadata (8.7 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.12/dist-packages (from sacrebleu==2.4.3) (2024.11.6)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.12/dist-packages (from sacrebleu==2.4.3) (0.9.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from sacrebleu==2.4.3) (2.0.2)\n",
            "Collecting colorama (from sacrebleu==2.4.3)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.12/dist-packages (from sacrebleu==2.4.3) (5.4.0)\n",
            "Downloading sacrebleu-2.4.3-py3-none-any.whl (103 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.0/104.0 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Downloading portalocker-3.2.0-py3-none-any.whl (22 kB)\n",
            "Installing collected packages: portalocker, colorama, sacrebleu\n",
            "Successfully installed colorama-0.4.6 portalocker-3.2.0 sacrebleu-2.4.3\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers[sentencepiece]==4.44.2 # 4.44.2\n",
        "!pip install datasets==2.19.0\n",
        "!pip install evaluate==0.4.3\n",
        "!pip install accelerate==0.34.2\n",
        "!pip install sacrebleu==2.4.3"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#txt to json\n",
        "import sys\n",
        "import json\n",
        "import re\n",
        "import codecs\n",
        "\n",
        "# json from huggingface\n",
        "#{ \"translation\": { \"en\": \"Others have dismissed him as a joke.\", \"ro\": \"Alții l-au numit o glumă.\" } }\n",
        "#{ \"translation\": { \"en\": \"And some are holding out for an implosion.\", \"ro\": \"Iar alții așteaptă implozia.\" } }\n",
        "\n",
        "def txt2json(src_id, trg_id, src_file, trg_file, out_file):\n",
        "\n",
        "  src = codecs.open(src_file, 'r', encoding=\"utf-8\")\n",
        "  trg = codecs.open(trg_file, 'r', encoding=\"utf-8\")\n",
        "  out_json = codecs.open(out_file, 'w', encoding=\"utf-8\")\n",
        "\n",
        "\n",
        "  src_lines = []\n",
        "  trg_lines = []\n",
        "  for line_s, line_t in zip(src, trg):\n",
        "      line_s = line_s.strip()\n",
        "      line_t = line_t.strip()\n",
        "      src_lines.append(line_s)\n",
        "      trg_lines.append(line_t)\n",
        "  recs = [src_lines, trg_lines]\n",
        "  for src, tgt in zip(*recs):\n",
        "      out = {\"translation\": { src_id: src, trg_id: tgt } }\n",
        "      x = json.dumps(out, indent=0, ensure_ascii=False)\n",
        "      x = re.sub(r'\\n', ' ', x, 0, re.M)\n",
        "      out_json.write(x + \"\\n\")\n",
        "  out_json.close()\n",
        "  return"
      ],
      "metadata": {
        "id": "70uOsNJMMnce"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NY1KF3TDVICb",
        "outputId": "9fa055ad-88f8-40f8-8319-0f21855434bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sample_data\t\t\t\ttrain.en-de.en-filtered.en.semantic.en\n",
            "train.en-de.de-filtered.de.semantic.de\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#lang_pair = \"en-de\"\n",
        "\n",
        "#lang_pair = \"en-de\"\n",
        "\n",
        "train_src = \"train.en-de.en-filtered.en.semantic.en\"\n",
        "train_trg = \"train.en-de.de-filtered.de.semantic.de\"\n",
        "train_json = \"train.en-de.json\"\n",
        "\n",
        "txt2json('en_XX', 'de_DE', train_src, train_trg, train_json)"
      ],
      "metadata": {
        "id": "LuaqXaq_M24N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "from typing import Callable, Dict, Iterable, List, Tuple, Union\n",
        "\n",
        "# Utilities for freezing parameters and checking whether they are frozen\n",
        "def lmap(f: Callable, x: Iterable) -> List:\n",
        "    \"\"\"list(map(f, x))\"\"\"\n",
        "    return list(map(f, x))\n",
        "\n",
        "def freeze_params(model: nn.Module): #inheritance from Model class\n",
        "    \"\"\"Set requires_grad=False for each of model.parameters()\"\"\"\n",
        "    for par in model.parameters(): #for each parameter in a model\n",
        "        par.requires_grad = False #freezing\n",
        "\n",
        "\n",
        "def freeze_embeds(model):\n",
        "    \"\"\"Freeze token embeddings and positional embeddings for bart, just token embeddings for t5.\"\"\"\n",
        "    model_type = model.config.model_type #to learn the type of the model\n",
        "\n",
        "    if model_type == \"t5\": #it has only tokens\n",
        "        freeze_params(model.shared)\n",
        "        for d in [model.encoder, model.decoder]:\n",
        "            freeze_params(d.embed_tokens)\n",
        "    elif model_type == \"fsmt\":\n",
        "        for d in [model.model.encoder, model.model.decoder]:\n",
        "            freeze_params(d.embed_positions)\n",
        "            freeze_params(d.embed_tokens)\n",
        "    else:\n",
        "        freeze_params(model.model.shared)  #mbart is here\n",
        "        for d in [model.model.encoder, model.model.decoder]:\n",
        "            freeze_params(d.embed_positions) #positional embeddings\n",
        "            freeze_params(d.embed_tokens) #token for the word\n",
        "\n",
        "\n",
        "def grad_status(model: nn.Module) -> Iterable:\n",
        "    return (par.requires_grad for par in model.parameters())\n",
        "\n",
        "\n",
        "def any_requires_grad(model: nn.Module) -> bool:\n",
        "    return any(grad_status(model))\n",
        "\n",
        "\n",
        "def assert_all_frozen(model): #to check if the model works\n",
        "    model_grads: List[bool] = list(grad_status(model))\n",
        "    n_require_grad = sum(lmap(int, model_grads))\n",
        "    npars = len(model_grads)\n",
        "    assert not any(model_grads), f\"{n_require_grad/npars:.1%} of {npars} weights require grad\"\n",
        "\n",
        "\n",
        "def assert_not_all_frozen(model):\n",
        "    model_grads: List[bool] = list(grad_status(model))\n",
        "    npars = len(model_grads)\n",
        "    assert any(model_grads), f\"none of {npars} weights require grad\""
      ],
      "metadata": {
        "id": "J-hKEDr4EGeN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import (\n",
        "    CONFIG_MAPPING,\n",
        "    MODEL_MAPPING,\n",
        "    AdamW,\n",
        "    AutoConfig,\n",
        "    AutoModelForSeq2SeqLM,\n",
        "    AutoTokenizer,\n",
        "    DataCollatorForSeq2Seq,\n",
        "    MBartTokenizer,\n",
        "    MBart50Tokenizer,\n",
        "    MBartTokenizerFast,\n",
        "    MBart50TokenizerFast,\n",
        "    SchedulerType,\n",
        "    default_data_collator,\n",
        "    get_scheduler,\n",
        "    set_seed,\n",
        ")\n",
        "from datasets import load_dataset, load_metric, DatasetDict\n",
        "import transformers\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "MODEL_CONFIG_CLASSES = list(MODEL_MAPPING.keys())\n",
        "MODEL_TYPES = tuple(conf.model_type for conf in MODEL_CONFIG_CLASSES)\n",
        "MULTILINGUAL_TOKENIZERS = [MBartTokenizer, MBartTokenizerFast, MBart50Tokenizer, MBart50TokenizerFast]\n",
        "\n",
        "\n",
        "def print_trainable_parameters(model):\n",
        "    \"\"\"\n",
        "    Prints the number of trainable parameters in the model.\n",
        "    \"\"\"\n",
        "    trainable_params = 0\n",
        "    all_param = 0\n",
        "    for _, param in model.named_parameters():\n",
        "        all_param += param.numel() #to count parameters that are not frozen\n",
        "        if param.requires_grad:\n",
        "            trainable_params += param.numel()\n",
        "    print(\n",
        "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
        "    )\n",
        "\n",
        "def main():\n",
        "\n",
        "    model_id = \"facebook/mbart-large-50-one-to-many-mmt\"\n",
        "    max_length = 100\n",
        "    freeze_embed = True\n",
        "    freeze_encoder = True\n",
        "\n",
        "    code2lang = {\n",
        "    \"de\": \"German\",\n",
        "    \"fr\": \"French\",\n",
        "    \"en\": \"English\",\n",
        "    \"nl\": \"Dutch\",\n",
        "    \"pt\": \"Portuguese\",\n",
        "    \"ru\": \"Russian\",\n",
        "    \"zh\": \"Chinese\",\n",
        "    \"ro\": \"Romanian\",\n",
        "    \"es\": \"Spanish\"\n",
        "    }\n",
        "    source_code = 'en_XX'\n",
        "    target_code = 'de_DE'\n",
        "    forced_bos_token = 'de_DE' #TODO!!!??? or empty str\"\"\n",
        "    data_files = \"train.en-de.json\"#\n",
        "    #https://arxiv.org/pdf/2312.12740.pdf trainig size 20k\n",
        "    output_dir = 'models/mbart50-finetune_freeze_emb_enc'\n",
        "    train_bs = 6 #bigger batch size\n",
        "    grad_acc = 4 #24 sentences in the batch\n",
        "    lr = 1e-4\n",
        "    w_steps = 0.03\n",
        "    n_epoch = 2\n",
        "    lr_scheduler_type = \"linear\" #'cosine', etc\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "    model = AutoModelForSeq2SeqLM.from_pretrained(model_id,\n",
        "                                                  device_map=\"auto\") #{\"\": 0} #to map to ressources\n",
        "\n",
        "    model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "    # Set decoder_start_token_id\n",
        "    if model.config.decoder_start_token_id is None and isinstance(tokenizer, (MBartTokenizer, MBartTokenizerFast)):\n",
        "      if isinstance(tokenizer, MBartTokenizer, MBartTokenizerFast):\n",
        "          model.config.decoder_start_token_id = tokenizer.lang_code_to_id[target_code] #Ru\n",
        "      else:\n",
        "          model.config.decoder_start_token_id = tokenizer.convert_tokens_to_ids(target_code)\n",
        "\n",
        "\n",
        "    if isinstance(tokenizer, tuple(MULTILINGUAL_TOKENIZERS)):\n",
        "\n",
        "      tokenizer.src_lang = source_code\n",
        "      tokenizer.tgt_lang = target_code\n",
        "\n",
        "      # For multilingual translation models like mBART-50 and M2M100 we need to force the target language token\n",
        "      # as the first generated token.\n",
        "      forced_bos_token_id = (\n",
        "          tokenizer.lang_code_to_id[forced_bos_token] if forced_bos_token is not None else None\n",
        "      )\n",
        "      model.config.forced_bos_token_id = forced_bos_token_id\n",
        "      #print(tokenizer.src_lang, model.config.forced_bos_token_id)\n",
        "\n",
        "\n",
        "  # do 3 versions (embed only, param only, both)\n",
        "    if freeze_embed:\n",
        "      print(\"***** freeze embeddings *****\")\n",
        "      freeze_embeds(model)\n",
        "\n",
        "    if freeze_encoder:\n",
        "      print(\"***** freeze encoder *****\")\n",
        "      freeze_params(model.get_encoder())\n",
        "      assert_all_frozen(model.get_encoder())\n",
        "\n",
        "    print_trainable_parameters(model)\n",
        "    print(model)\n",
        "\n",
        "    metric = load_metric('sacrebleu', trust_remote_code=True)\n",
        "\n",
        "    def preprocess_parallel_function(examples):\n",
        "      inputs = [ex[source_code] for ex in examples[\"translation\"]]\n",
        "      targets = [ex[target_code] for ex in examples[\"translation\"]]\n",
        "      #inputs = [prefix + inp for inp in inputs]\n",
        "      #print(inputs, targets)\n",
        "      model_inputs = tokenizer(inputs, max_length=max_length, padding=False, truncation=True)\n",
        "\n",
        "\n",
        "      labels = tokenizer(targets, max_length=max_length, padding=False, truncation=True)\n",
        "\n",
        "      #if padding == \"max_length\" and ignore_pad_token_for_loss:\n",
        "      #labels[\"input_ids\"] = [\n",
        "      #    [(l if l != tokenizer.pad_token_id else -100) for l in label]\n",
        "      #    for label in labels[\"input_ids\"]]\n",
        "\n",
        "\n",
        "\n",
        "      model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "\n",
        "      return model_inputs\n",
        "\n",
        "    def postprocess_text(preds, labels):\n",
        "      preds = [pred.strip() for pred in preds]\n",
        "      labels = [[label.strip()] for label in labels]\n",
        "\n",
        "      return preds, labels\n",
        "\n",
        "    def compute_metrics(eval_preds, ignore_pad_token_for_loss=False):\n",
        "      preds, labels = eval_preds\n",
        "      if isinstance(preds, tuple):\n",
        "        preds = preds[0]\n",
        "      decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
        "      # Replace -100 in the labels as we can't decode them.\n",
        "      labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "      decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "      # Some simple post-processing\n",
        "      decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
        "\n",
        "      result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
        "      prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
        "      result = {'bleu' : result['score']}\n",
        "      result[\"gen_len\"] = np.mean(prediction_lens)\n",
        "      result = {k: round(v, 4) for k, v in result.items()}\n",
        "      return result\n",
        "\n",
        "\n",
        "    #data = load_from_disk(\"path_directory\")\n",
        "    #put data_save_folder from drive\n",
        "    data = load_dataset(\"json\", data_files=data_files)\n",
        "    data_split_tmp = data['train'].train_test_split(test_size=0.2, seed=42)\n",
        "    data_split_valid_test = data_split_tmp['test'].train_test_split(test_size=0.5, seed=42)\n",
        "    data = DatasetDict({'train': data_split_tmp['train'].select(range(0, 5000)), #check your credits!!!\n",
        "                        'valid': data_split_valid_test['train'].select(range(0, 200)),\n",
        "                        'test':  data_split_valid_test['test'].select(range(0, 200)),})\n",
        "    #data['test']\n",
        "    column_names = data[\"train\"].column_names\n",
        "    #print(column_names)\n",
        "    data = data.map(preprocess_parallel_function,\n",
        "                    batched=True)\n",
        "    label_pad_token_id = -100 #TODO??\n",
        "\n",
        "    trainer = transformers.Seq2SeqTrainer(\n",
        "        model=model,\n",
        "        train_dataset=data[\"train\"],\n",
        "        eval_dataset=data[\"valid\"],\n",
        "        args=transformers.Seq2SeqTrainingArguments(\n",
        "            report_to='none', #turn off  wandb\n",
        "            per_device_train_batch_size=train_bs, #4, 12\n",
        "            gradient_accumulation_steps=grad_acc,\n",
        "            do_train=True,\n",
        "            do_eval=True,\n",
        "            per_device_eval_batch_size=4,\n",
        "            eval_accumulation_steps=2,\n",
        "            warmup_ratio=w_steps,\n",
        "            lr_scheduler_type=lr_scheduler_type,\n",
        "            num_train_epochs=n_epoch, #5?\n",
        "            predict_with_generate=True,\n",
        "            metric_for_best_model='bleu',\n",
        "            load_best_model_at_end=True,\n",
        "            learning_rate=lr, #0.1, 0.01, 0.001\n",
        "            save_total_limit=1,\n",
        "            generation_num_beams=4,\n",
        "            save_strategy=\"epoch\",\n",
        "            eval_strategy=\"epoch\",\n",
        "            output_dir=output_dir,\n",
        "        ),\n",
        "        data_collator=transformers.DataCollatorForSeq2Seq(tokenizer,\n",
        "                                                          label_pad_token_id=label_pad_token_id,\n",
        "                                                          model=model),\n",
        "        compute_metrics=compute_metrics,\n",
        "        tokenizer=tokenizer,\n",
        "    )\n",
        "    model.config.use_cache = False  # silence the warnings. Please re-enable for inference!\n",
        "    trainer.train()\n",
        "\n",
        "    trainer.save_model(output_dir)\n",
        "    tokenizer.save_pretrained(output_dir)\n"
      ],
      "metadata": {
        "id": "d7fC4fxTPMhb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#fine-tune\n",
        "main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "1c337fcc0f344653bfcf1184e819e784",
            "d675460a1f004c7cb75141c3677d243c",
            "879253a5ebc941c787ea1f79297bca39",
            "2bbf3280f99048058d038270cc909c9b",
            "729d2cdf2a1c4b4583ac8ee8dc69f545",
            "ef9c26fa51174462bb48927b0cec7c77",
            "5fad5b2068934a77838bf7df7da49666",
            "178e42b7b8e04ad89d59819dc7166df2",
            "32415da382cf431488c19e373e761f55",
            "ad42cea1987f4060b59afadb37362c64",
            "2561a930ee0a4886a173ce8a92567d19",
            "7997d8194f51436b8b66cb1ed90991cb",
            "2c1a26923c504c7089bb5ae7f7a2db79",
            "181e3bad4b774b9cb34ea064c04568f5",
            "fc01c1c42747444ea9bf1ec41e91e1ff",
            "6c86ab33fcaf4f8daf21e4f5e53d2f1f",
            "f214a7bfd99c415aa74614fcc62b87fd",
            "83993188f3ef422dbe04b580a38297e5",
            "5a5121b5285742cb9d254493fab35233",
            "dd0da857fa2b4eb6916764e10239642b",
            "faaa5163e33743418a0711af12a40951",
            "1fb8ebdafbed419c87d4b4a1fe1ab7ba",
            "6a87442948f644c894c4502f29c91e96",
            "1cd94ec6b6034578bbef09e0f7921f6e",
            "c16ce12516544536807017369067e1d3",
            "51455094a8d14868ad28e9a238951074",
            "d362804d450a437ebb58c79209ce55d2",
            "4cae0939344642448206d9b1fd8c92fc",
            "db964ff65cbb41da8727d10e031cddad",
            "54a991f97a9046a0be675b5edcf1f5db",
            "291bf5c1d9ca4369b8438d5d945978b9",
            "8d0b6f6d8d51413bb7645e154698d8b2",
            "c4ac98306d91473cb043f52f7b744a24"
          ]
        },
        "id": "-rgHNp1MRgrc",
        "outputId": "662dd456-a50b-438c-d637-3a63a2f50315"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "***** freeze embeddings *****\n",
            "***** freeze encoder *****\n",
            "trainable params: 201564160 || all params: 610879488 || trainable%: 32.99573221224282\n",
            "MBartForConditionalGeneration(\n",
            "  (model): MBartModel(\n",
            "    (shared): MBartScaledWordEmbedding(250054, 1024, padding_idx=1)\n",
            "    (encoder): MBartEncoder(\n",
            "      (embed_tokens): MBartScaledWordEmbedding(250054, 1024, padding_idx=1)\n",
            "      (embed_positions): MBartLearnedPositionalEmbedding(1026, 1024)\n",
            "      (layers): ModuleList(\n",
            "        (0-11): 12 x MBartEncoderLayer(\n",
            "          (self_attn): MBartAttention(\n",
            "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          )\n",
            "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "          (activation_fn): ReLU()\n",
            "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "      )\n",
            "      (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "    )\n",
            "    (decoder): MBartDecoder(\n",
            "      (embed_tokens): MBartScaledWordEmbedding(250054, 1024, padding_idx=1)\n",
            "      (embed_positions): MBartLearnedPositionalEmbedding(1026, 1024)\n",
            "      (layers): ModuleList(\n",
            "        (0-11): 12 x MBartDecoderLayer(\n",
            "          (self_attn): MBartAttention(\n",
            "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          )\n",
            "          (activation_fn): ReLU()\n",
            "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "          (encoder_attn): MBartAttention(\n",
            "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          )\n",
            "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "      )\n",
            "      (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "    )\n",
            "  )\n",
            "  (lm_head): Linear(in_features=1024, out_features=250054, bias=False)\n",
            ")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2109429943.py:114: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
            "  metric = load_metric('sacrebleu', trust_remote_code=True)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/5000 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1c337fcc0f344653bfcf1184e819e784"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7997d8194f51436b8b66cb1ed90991cb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6a87442948f644c894c4502f29c91e96"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='416' max='416' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [416/416 32:23, Epoch 1/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Bleu</th>\n",
              "      <th>Gen Len</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.885356</td>\n",
              "      <td>41.288600</td>\n",
              "      <td>32.465000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.856925</td>\n",
              "      <td>42.677100</td>\n",
              "      <td>31.655000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/transformers/generation/utils.py:1375: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
            "  warnings.warn(\n",
            "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
            "Non-default generation parameters: {'max_length': 200, 'early_stopping': True, 'num_beams': 5, 'forced_bos_token_id': 250003, 'forced_eos_token_id': 2}\n",
            "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
            "Non-default generation parameters: {'max_length': 200, 'early_stopping': True, 'num_beams': 5, 'forced_bos_token_id': 250003, 'forced_eos_token_id': 2}\n",
            "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
            "Non-default generation parameters: {'max_length': 200, 'early_stopping': True, 'num_beams': 5, 'forced_bos_token_id': 250003, 'forced_eos_token_id': 2}\n",
            "There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.decoder.embed_tokens.weight', 'lm_head.weight'].\n",
            "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
            "Non-default generation parameters: {'max_length': 200, 'early_stopping': True, 'num_beams': 5, 'forced_bos_token_id': 250003, 'forced_eos_token_id': 2}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!cp -r models/mbart50_finetune drive/"
      ],
      "metadata": {
        "id": "-FmccjBTiJP4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "id": "0yl4cExribpt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!tar -czvf mbart50-finetune.tar.gz models/mbart50-finietune"
      ],
      "metadata": {
        "id": "nrhsjqmdik2y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "#del(model)\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "ETHQvMZK1S5u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Freezed Fine-Tuning with my Corpus\n",
        "\n",
        "...."
      ],
      "metadata": {
        "id": "idSgPRJICXmV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Only Embeddings"
      ],
      "metadata": {
        "id": "5MICGktnDCiN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "from typing import Callable, Dict, Iterable, List, Tuple, Union\n",
        "\n",
        "# Utilities for freezing parameters and checking whether they are frozen\n",
        "def lmap(f: Callable, x: Iterable) -> List:\n",
        "    \"\"\"list(map(f, x))\"\"\"\n",
        "    return list(map(f, x))\n",
        "\n",
        "def freeze_params(model: nn.Module): #inheritance from Model class\n",
        "    \"\"\"Set requires_grad=False for each of model.parameters()\"\"\"\n",
        "    for par in model.parameters(): #for each parameter in a model\n",
        "        par.requires_grad = False #freezing\n",
        "\n",
        "\n",
        "def freeze_embeds(model):\n",
        "    \"\"\"Freeze token embeddings and positional embeddings for bart, just token embeddings for t5.\"\"\"\n",
        "    model_type = model.config.model_type #to learn the type of the model\n",
        "\n",
        "    if model_type == \"t5\": #it has only tokens\n",
        "        freeze_params(model.shared)\n",
        "        for d in [model.encoder, model.decoder]:\n",
        "            freeze_params(d.embed_tokens)\n",
        "    elif model_type == \"fsmt\":\n",
        "        for d in [model.model.encoder, model.model.decoder]:\n",
        "            freeze_params(d.embed_positions)\n",
        "            freeze_params(d.embed_tokens)\n",
        "    else:\n",
        "        freeze_params(model.model.shared)  #mbart is here\n",
        "        for d in [model.model.encoder, model.model.decoder]:\n",
        "            freeze_params(d.embed_positions) #positional embeddings\n",
        "            freeze_params(d.embed_tokens) #token for the word\n",
        "\n",
        "\n",
        "def grad_status(model: nn.Module) -> Iterable:\n",
        "    return (par.requires_grad for par in model.parameters())\n",
        "\n",
        "\n",
        "def any_requires_grad(model: nn.Module) -> bool:\n",
        "    return any(grad_status(model))\n",
        "\n",
        "\n",
        "def assert_all_frozen(model): #to check if the model works\n",
        "    model_grads: List[bool] = list(grad_status(model))\n",
        "    n_require_grad = sum(lmap(int, model_grads))\n",
        "    npars = len(model_grads)\n",
        "    assert not any(model_grads), f\"{n_require_grad/npars:.1%} of {npars} weights require grad\"\n",
        "\n",
        "\n",
        "def assert_not_all_frozen(model):\n",
        "    model_grads: List[bool] = list(grad_status(model))\n",
        "    npars = len(model_grads)\n",
        "    assert any(model_grads), f\"none of {npars} weights require grad\""
      ],
      "metadata": {
        "id": "GVXJQ7GHCm0q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import (\n",
        "    CONFIG_MAPPING,\n",
        "    MODEL_MAPPING,\n",
        "    AdamW,\n",
        "    AutoConfig,\n",
        "    AutoModelForSeq2SeqLM,\n",
        "    AutoTokenizer,\n",
        "    DataCollatorForSeq2Seq,\n",
        "    MBartTokenizer,\n",
        "    MBart50Tokenizer,\n",
        "    MBartTokenizerFast,\n",
        "    MBart50TokenizerFast,\n",
        "    SchedulerType,\n",
        "    default_data_collator,\n",
        "    get_scheduler,\n",
        "    set_seed,\n",
        ")\n",
        "from datasets import load_dataset, load_metric, DatasetDict\n",
        "import transformers\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "MODEL_CONFIG_CLASSES = list(MODEL_MAPPING.keys())\n",
        "MODEL_TYPES = tuple(conf.model_type for conf in MODEL_CONFIG_CLASSES)\n",
        "MULTILINGUAL_TOKENIZERS = [MBartTokenizer, MBartTokenizerFast, MBart50Tokenizer, MBart50TokenizerFast]\n",
        "\n",
        "\n",
        "def print_trainable_parameters(model):\n",
        "    \"\"\"\n",
        "    Prints the number of trainable parameters in the model.\n",
        "    \"\"\"\n",
        "    trainable_params = 0\n",
        "    all_param = 0\n",
        "    for _, param in model.named_parameters():\n",
        "        all_param += param.numel() #to count parameters that are not frozen\n",
        "        if param.requires_grad:\n",
        "            trainable_params += param.numel()\n",
        "    print(\n",
        "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
        "    )\n",
        "\n",
        "def main():\n",
        "\n",
        "    model_id = \"facebook/mbart-large-50-one-to-many-mmt\"\n",
        "    max_length = 100\n",
        "    freeze_embed = True\n",
        "    freeze_encoder = True\n",
        "\n",
        "    code2lang = {\n",
        "    \"en\": \"English\",\n",
        "    \"ru\": \"Russian\",\n",
        "    }\n",
        "\n",
        "    source_code = 'en_XX'\n",
        "    target_code = 'ru_RU'\n",
        "    forced_bos_token = 'ru_RU'\n",
        "    data_files = \"train.en-ru.json\" #https://arxiv.org/pdf/2312.12740.pdf trainig size 20k\n",
        "    output_dir = 'models/mbart50-finetune_freeze_emb_enc'\n",
        "    train_bs = 6 #bigger batch size\n",
        "    grad_acc = 4 #24 sentences in the batch\n",
        "    lr = 1e-4\n",
        "    w_steps = 0.03\n",
        "    n_epoch = 2\n",
        "    lr_scheduler_type = \"linear\" #'cosine', etc\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "    model = AutoModelForSeq2SeqLM.from_pretrained(model_id,\n",
        "                                                  device_map=\"auto\") #{\"\": 0} #to map to ressources\n",
        "\n",
        "    model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "    # Set decoder_start_token_id\n",
        "    if model.config.decoder_start_token_id is None and isinstance(tokenizer, (MBartTokenizer, MBartTokenizerFast)):\n",
        "      if isinstance(tokenizer, MBartTokenizer, MBartTokenizerFast):\n",
        "          model.config.decoder_start_token_id = tokenizer.lang_code_to_id[target_code] #Ru\n",
        "      else:\n",
        "          model.config.decoder_start_token_id = tokenizer.convert_tokens_to_ids(target_code)\n",
        "\n",
        "\n",
        "    if isinstance(tokenizer, tuple(MULTILINGUAL_TOKENIZERS)):\n",
        "\n",
        "      tokenizer.src_lang = source_code\n",
        "      tokenizer.tgt_lang = target_code\n",
        "\n",
        "      # For multilingual translation models like mBART-50 and M2M100 we need to force the target language token\n",
        "      # as the first generated token.\n",
        "      forced_bos_token_id = (\n",
        "          tokenizer.lang_code_to_id[forced_bos_token] if forced_bos_token is not None else None\n",
        "      )\n",
        "      model.config.forced_bos_token_id = forced_bos_token_id\n",
        "      #print(tokenizer.src_lang, model.config.forced_bos_token_id)\n",
        "\n",
        "\n",
        "  # do 3 versions (embed only, param only, both)\n",
        "    if freeze_embed:\n",
        "      print(\"***** freeze embeddings *****\")\n",
        "      freeze_embeds(model)\n",
        "\n",
        "    if freeze_encoder:\n",
        "      print(\"***** freeze encoder *****\")\n",
        "      freeze_params(model.get_encoder())\n",
        "      assert_all_frozen(model.get_encoder())\n",
        "\n",
        "    print_trainable_parameters(model)\n",
        "    print(model)\n",
        "\n",
        "    metric = load_metric('sacrebleu', trust_remote_code=True)\n",
        "\n",
        "    def preprocess_parallel_function(examples):\n",
        "      inputs = [ex[source_code] for ex in examples[\"translation\"]]\n",
        "      targets = [ex[target_code] for ex in examples[\"translation\"]]\n",
        "      #inputs = [prefix + inp for inp in inputs]\n",
        "      #print(inputs, targets)\n",
        "      model_inputs = tokenizer(inputs, max_length=max_length, padding=False, truncation=True)\n",
        "\n",
        "\n",
        "      labels = tokenizer(targets, max_length=max_length, padding=False, truncation=True)\n",
        "\n",
        "      #if padding == \"max_length\" and ignore_pad_token_for_loss:\n",
        "      #labels[\"input_ids\"] = [\n",
        "      #    [(l if l != tokenizer.pad_token_id else -100) for l in label]\n",
        "      #    for label in labels[\"input_ids\"]]\n",
        "\n",
        "\n",
        "      model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "\n",
        "      return model_inputs\n",
        "\n",
        "    def postprocess_text(preds, labels):\n",
        "      preds = [pred.strip() for pred in preds]\n",
        "      labels = [[label.strip()] for label in labels]\n",
        "\n",
        "      return preds, labels\n",
        "\n",
        "    def compute_metrics(eval_preds, ignore_pad_token_for_loss=False):\n",
        "      preds, labels = eval_preds\n",
        "      if isinstance(preds, tuple):\n",
        "        preds = preds[0]\n",
        "      decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
        "      # Replace -100 in the labels as we can't decode them.\n",
        "      labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "      decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "      # Some simple post-processing\n",
        "      decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
        "\n",
        "      result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
        "      prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
        "      result = {'bleu' : result['score']}\n",
        "      result[\"gen_len\"] = np.mean(prediction_lens)\n",
        "      result = {k: round(v, 4) for k, v in result.items()}\n",
        "      return result\n",
        "\n",
        "\n",
        "    #data = load_from_disk(\"path_directory\")\n",
        "    #put data_save_folder from drive\n",
        "    data = load_dataset(\"json\", data_files=data_files)\n",
        "    data_split_tmp = data['train'].train_test_split(test_size=0.2, seed=42)\n",
        "    data_split_valid_test = data_split_tmp['test'].train_test_split(test_size=0.5, seed=42)\n",
        "    data = DatasetDict({'train': data_split_tmp['train'].select(range(0, 5000)), #check your credits!!!\n",
        "                        'valid': data_split_valid_test['train'].select(range(0, 200)),\n",
        "                        'test':  data_split_valid_test['test'].select(range(0, 200)),})\n",
        "    #data['test']\n",
        "    column_names = data[\"train\"].column_names\n",
        "    #print(column_names)\n",
        "    data = data.map(preprocess_parallel_function,\n",
        "                    batched=True)\n",
        "    label_pad_token_id = -100\n",
        "\n",
        "    trainer = transformers.Seq2SeqTrainer(\n",
        "        model=model,\n",
        "        train_dataset=data[\"train\"],\n",
        "        eval_dataset=data[\"valid\"],\n",
        "        args=transformers.Seq2SeqTrainingArguments(\n",
        "            report_to='none', #turn off  wandb\n",
        "            per_device_train_batch_size=train_bs, #4, 12\n",
        "            gradient_accumulation_steps=grad_acc,\n",
        "            do_train=True,\n",
        "            do_eval=True,\n",
        "            per_device_eval_batch_size=4,\n",
        "            eval_accumulation_steps=2,\n",
        "            warmup_ratio=w_steps,\n",
        "            lr_scheduler_type=lr_scheduler_type,\n",
        "            num_train_epochs=n_epoch, #5?\n",
        "            predict_with_generate=True,\n",
        "            metric_for_best_model='bleu',\n",
        "            load_best_model_at_end=True,\n",
        "            learning_rate=lr, #0.1, 0.01, 0.001\n",
        "            save_total_limit=1,\n",
        "            generation_num_beams=4,\n",
        "            save_strategy=\"epoch\",\n",
        "            eval_strategy=\"epoch\",\n",
        "            output_dir=output_dir,\n",
        "        ),\n",
        "        data_collator=transformers.DataCollatorForSeq2Seq(tokenizer,\n",
        "                                                          label_pad_token_id=label_pad_token_id,\n",
        "                                                          model=model),\n",
        "        compute_metrics=compute_metrics,\n",
        "        tokenizer=tokenizer,\n",
        "    )\n",
        "    model.config.use_cache = False  # silence the warnings. Please re-enable for inference!\n",
        "    trainer.train()\n",
        "\n",
        "    trainer.save_model(output_dir)\n",
        "    tokenizer.save_pretrained(output_dir)"
      ],
      "metadata": {
        "id": "LoQVgLVJCm0r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#fine-tune\n",
        "main(freeze_embed = True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "1c337fcc0f344653bfcf1184e819e784",
            "d675460a1f004c7cb75141c3677d243c",
            "879253a5ebc941c787ea1f79297bca39",
            "2bbf3280f99048058d038270cc909c9b",
            "729d2cdf2a1c4b4583ac8ee8dc69f545",
            "ef9c26fa51174462bb48927b0cec7c77",
            "5fad5b2068934a77838bf7df7da49666",
            "178e42b7b8e04ad89d59819dc7166df2",
            "32415da382cf431488c19e373e761f55",
            "ad42cea1987f4060b59afadb37362c64",
            "2561a930ee0a4886a173ce8a92567d19",
            "7997d8194f51436b8b66cb1ed90991cb",
            "2c1a26923c504c7089bb5ae7f7a2db79",
            "181e3bad4b774b9cb34ea064c04568f5",
            "fc01c1c42747444ea9bf1ec41e91e1ff",
            "6c86ab33fcaf4f8daf21e4f5e53d2f1f",
            "f214a7bfd99c415aa74614fcc62b87fd",
            "83993188f3ef422dbe04b580a38297e5",
            "5a5121b5285742cb9d254493fab35233",
            "dd0da857fa2b4eb6916764e10239642b",
            "faaa5163e33743418a0711af12a40951",
            "1fb8ebdafbed419c87d4b4a1fe1ab7ba",
            "6a87442948f644c894c4502f29c91e96",
            "1cd94ec6b6034578bbef09e0f7921f6e",
            "c16ce12516544536807017369067e1d3",
            "51455094a8d14868ad28e9a238951074",
            "d362804d450a437ebb58c79209ce55d2",
            "4cae0939344642448206d9b1fd8c92fc",
            "db964ff65cbb41da8727d10e031cddad",
            "54a991f97a9046a0be675b5edcf1f5db",
            "291bf5c1d9ca4369b8438d5d945978b9",
            "8d0b6f6d8d51413bb7645e154698d8b2",
            "c4ac98306d91473cb043f52f7b744a24"
          ]
        },
        "outputId": "662dd456-a50b-438c-d637-3a63a2f50315",
        "id": "x8QQtPxACm0s"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "***** freeze embeddings *****\n",
            "***** freeze encoder *****\n",
            "trainable params: 201564160 || all params: 610879488 || trainable%: 32.99573221224282\n",
            "MBartForConditionalGeneration(\n",
            "  (model): MBartModel(\n",
            "    (shared): MBartScaledWordEmbedding(250054, 1024, padding_idx=1)\n",
            "    (encoder): MBartEncoder(\n",
            "      (embed_tokens): MBartScaledWordEmbedding(250054, 1024, padding_idx=1)\n",
            "      (embed_positions): MBartLearnedPositionalEmbedding(1026, 1024)\n",
            "      (layers): ModuleList(\n",
            "        (0-11): 12 x MBartEncoderLayer(\n",
            "          (self_attn): MBartAttention(\n",
            "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          )\n",
            "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "          (activation_fn): ReLU()\n",
            "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "      )\n",
            "      (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "    )\n",
            "    (decoder): MBartDecoder(\n",
            "      (embed_tokens): MBartScaledWordEmbedding(250054, 1024, padding_idx=1)\n",
            "      (embed_positions): MBartLearnedPositionalEmbedding(1026, 1024)\n",
            "      (layers): ModuleList(\n",
            "        (0-11): 12 x MBartDecoderLayer(\n",
            "          (self_attn): MBartAttention(\n",
            "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          )\n",
            "          (activation_fn): ReLU()\n",
            "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "          (encoder_attn): MBartAttention(\n",
            "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          )\n",
            "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "      )\n",
            "      (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "    )\n",
            "  )\n",
            "  (lm_head): Linear(in_features=1024, out_features=250054, bias=False)\n",
            ")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2109429943.py:114: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
            "  metric = load_metric('sacrebleu', trust_remote_code=True)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/5000 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1c337fcc0f344653bfcf1184e819e784"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7997d8194f51436b8b66cb1ed90991cb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6a87442948f644c894c4502f29c91e96"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='416' max='416' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [416/416 32:23, Epoch 1/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Bleu</th>\n",
              "      <th>Gen Len</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.885356</td>\n",
              "      <td>41.288600</td>\n",
              "      <td>32.465000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.856925</td>\n",
              "      <td>42.677100</td>\n",
              "      <td>31.655000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/transformers/generation/utils.py:1375: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
            "  warnings.warn(\n",
            "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
            "Non-default generation parameters: {'max_length': 200, 'early_stopping': True, 'num_beams': 5, 'forced_bos_token_id': 250003, 'forced_eos_token_id': 2}\n",
            "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
            "Non-default generation parameters: {'max_length': 200, 'early_stopping': True, 'num_beams': 5, 'forced_bos_token_id': 250003, 'forced_eos_token_id': 2}\n",
            "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
            "Non-default generation parameters: {'max_length': 200, 'early_stopping': True, 'num_beams': 5, 'forced_bos_token_id': 250003, 'forced_eos_token_id': 2}\n",
            "There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.decoder.embed_tokens.weight', 'lm_head.weight'].\n",
            "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
            "Non-default generation parameters: {'max_length': 200, 'early_stopping': True, 'num_beams': 5, 'forced_bos_token_id': 250003, 'forced_eos_token_id': 2}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#use models\n",
        "#import models and tokenizer\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "from datasets import load_from_disk\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"models/mbart50-full-finetune-1epoch-1e4\") #path of your directory with the finetuned\n",
        "\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\"models/mbart50-full-finetune-1epoch-1e4\") #gpu!!!\n",
        "\n",
        "#upload source file and read\n",
        "import codecs\n",
        "#file_name = \"Vienna_Environmental.en-de.valid.en\"\n",
        "mt_output = []\n",
        "output = open('mtouput_fullFT', 'w', encoding='utf-8')\n",
        "data = load_from_disk(\"data_save_test\")\n",
        "print(data)\n",
        "data['test']\n",
        "#with codecs.open(file_name, 'r', 'utf-8') as src:\n",
        "for line in data['test']: #trasn src and trg translation src and trg\n",
        "    src = line['translation']['en_XX']\n",
        "    trg = line['translation']['ru_RU']\n",
        "    #line = line.strip()\n",
        "    encoded = tokenizer(src, return_tensors=\"pt\")              #gpu?? tokenize\n",
        "    generated_tokens = model.generate(**encoded,\n",
        "                                      forced_bos_token_id=tokenizer.lang_code_to_id['ru_RU']) #add beam search 4\n",
        "    translation = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True) #de tokenize\n",
        "    print(translation[0], file=output)\n",
        "    print(translation[0])"
      ],
      "metadata": {
        "id": "7el0-hAvFJE7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Only encoder"
      ],
      "metadata": {
        "id": "Nc4Zocb1DHoN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#fine-tune\n",
        "main(freeze_encoder = True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "1c337fcc0f344653bfcf1184e819e784",
            "d675460a1f004c7cb75141c3677d243c",
            "879253a5ebc941c787ea1f79297bca39",
            "2bbf3280f99048058d038270cc909c9b",
            "729d2cdf2a1c4b4583ac8ee8dc69f545",
            "ef9c26fa51174462bb48927b0cec7c77",
            "5fad5b2068934a77838bf7df7da49666",
            "178e42b7b8e04ad89d59819dc7166df2",
            "32415da382cf431488c19e373e761f55",
            "ad42cea1987f4060b59afadb37362c64",
            "2561a930ee0a4886a173ce8a92567d19",
            "7997d8194f51436b8b66cb1ed90991cb",
            "2c1a26923c504c7089bb5ae7f7a2db79",
            "181e3bad4b774b9cb34ea064c04568f5",
            "fc01c1c42747444ea9bf1ec41e91e1ff",
            "6c86ab33fcaf4f8daf21e4f5e53d2f1f",
            "f214a7bfd99c415aa74614fcc62b87fd",
            "83993188f3ef422dbe04b580a38297e5",
            "5a5121b5285742cb9d254493fab35233",
            "dd0da857fa2b4eb6916764e10239642b",
            "faaa5163e33743418a0711af12a40951",
            "1fb8ebdafbed419c87d4b4a1fe1ab7ba",
            "6a87442948f644c894c4502f29c91e96",
            "1cd94ec6b6034578bbef09e0f7921f6e",
            "c16ce12516544536807017369067e1d3",
            "51455094a8d14868ad28e9a238951074",
            "d362804d450a437ebb58c79209ce55d2",
            "4cae0939344642448206d9b1fd8c92fc",
            "db964ff65cbb41da8727d10e031cddad",
            "54a991f97a9046a0be675b5edcf1f5db",
            "291bf5c1d9ca4369b8438d5d945978b9",
            "8d0b6f6d8d51413bb7645e154698d8b2",
            "c4ac98306d91473cb043f52f7b744a24"
          ]
        },
        "outputId": "662dd456-a50b-438c-d637-3a63a2f50315",
        "id": "_0vzgqI_DIAP"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "***** freeze embeddings *****\n",
            "***** freeze encoder *****\n",
            "trainable params: 201564160 || all params: 610879488 || trainable%: 32.99573221224282\n",
            "MBartForConditionalGeneration(\n",
            "  (model): MBartModel(\n",
            "    (shared): MBartScaledWordEmbedding(250054, 1024, padding_idx=1)\n",
            "    (encoder): MBartEncoder(\n",
            "      (embed_tokens): MBartScaledWordEmbedding(250054, 1024, padding_idx=1)\n",
            "      (embed_positions): MBartLearnedPositionalEmbedding(1026, 1024)\n",
            "      (layers): ModuleList(\n",
            "        (0-11): 12 x MBartEncoderLayer(\n",
            "          (self_attn): MBartAttention(\n",
            "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          )\n",
            "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "          (activation_fn): ReLU()\n",
            "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "      )\n",
            "      (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "    )\n",
            "    (decoder): MBartDecoder(\n",
            "      (embed_tokens): MBartScaledWordEmbedding(250054, 1024, padding_idx=1)\n",
            "      (embed_positions): MBartLearnedPositionalEmbedding(1026, 1024)\n",
            "      (layers): ModuleList(\n",
            "        (0-11): 12 x MBartDecoderLayer(\n",
            "          (self_attn): MBartAttention(\n",
            "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          )\n",
            "          (activation_fn): ReLU()\n",
            "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "          (encoder_attn): MBartAttention(\n",
            "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          )\n",
            "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "      )\n",
            "      (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "    )\n",
            "  )\n",
            "  (lm_head): Linear(in_features=1024, out_features=250054, bias=False)\n",
            ")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2109429943.py:114: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
            "  metric = load_metric('sacrebleu', trust_remote_code=True)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/5000 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1c337fcc0f344653bfcf1184e819e784"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7997d8194f51436b8b66cb1ed90991cb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6a87442948f644c894c4502f29c91e96"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='416' max='416' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [416/416 32:23, Epoch 1/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Bleu</th>\n",
              "      <th>Gen Len</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.885356</td>\n",
              "      <td>41.288600</td>\n",
              "      <td>32.465000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.856925</td>\n",
              "      <td>42.677100</td>\n",
              "      <td>31.655000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/transformers/generation/utils.py:1375: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
            "  warnings.warn(\n",
            "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
            "Non-default generation parameters: {'max_length': 200, 'early_stopping': True, 'num_beams': 5, 'forced_bos_token_id': 250003, 'forced_eos_token_id': 2}\n",
            "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
            "Non-default generation parameters: {'max_length': 200, 'early_stopping': True, 'num_beams': 5, 'forced_bos_token_id': 250003, 'forced_eos_token_id': 2}\n",
            "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
            "Non-default generation parameters: {'max_length': 200, 'early_stopping': True, 'num_beams': 5, 'forced_bos_token_id': 250003, 'forced_eos_token_id': 2}\n",
            "There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.decoder.embed_tokens.weight', 'lm_head.weight'].\n",
            "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
            "Non-default generation parameters: {'max_length': 200, 'early_stopping': True, 'num_beams': 5, 'forced_bos_token_id': 250003, 'forced_eos_token_id': 2}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#use models\n",
        "#import models and tokenizer\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "from datasets import load_from_disk\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"models/mbart50-full-finetune-1epoch-1e4\") #path of your directory with the finetuned\n",
        "\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\"models/mbart50-full-finetune-1epoch-1e4\") #gpu!!!\n",
        "\n",
        "#upload source file and read\n",
        "import codecs\n",
        "#file_name = \"Vienna_Environmental.en-de.valid.en\"\n",
        "mt_output = []\n",
        "output = open('mtouput_fullFT', 'w', encoding='utf-8')\n",
        "data = load_from_disk(\"data_save_test\")\n",
        "print(data)\n",
        "data['test']\n",
        "#with codecs.open(file_name, 'r', 'utf-8') as src:\n",
        "for line in data['test']: #trasn src and trg translation src and trg\n",
        "    src = line['translation']['en_XX']\n",
        "    trg = line['translation']['ru_RU']\n",
        "    #line = line.strip()\n",
        "    encoded = tokenizer(src, return_tensors=\"pt\")              #gpu?? tokenize\n",
        "    generated_tokens = model.generate(**encoded,\n",
        "                                      forced_bos_token_id=tokenizer.lang_code_to_id['ru_RU']) #add beam search 4\n",
        "    translation = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True) #de tokenize\n",
        "    print(translation[0], file=output)\n",
        "    print(translation[0])"
      ],
      "metadata": {
        "id": "-1ad2DqnFH0T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Both"
      ],
      "metadata": {
        "id": "FRnz4WYxDf3c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#fine-tune\n",
        "main(freeze_encoder = True, freeze_embed = True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "1c337fcc0f344653bfcf1184e819e784",
            "d675460a1f004c7cb75141c3677d243c",
            "879253a5ebc941c787ea1f79297bca39",
            "2bbf3280f99048058d038270cc909c9b",
            "729d2cdf2a1c4b4583ac8ee8dc69f545",
            "ef9c26fa51174462bb48927b0cec7c77",
            "5fad5b2068934a77838bf7df7da49666",
            "178e42b7b8e04ad89d59819dc7166df2",
            "32415da382cf431488c19e373e761f55",
            "ad42cea1987f4060b59afadb37362c64",
            "2561a930ee0a4886a173ce8a92567d19",
            "7997d8194f51436b8b66cb1ed90991cb",
            "2c1a26923c504c7089bb5ae7f7a2db79",
            "181e3bad4b774b9cb34ea064c04568f5",
            "fc01c1c42747444ea9bf1ec41e91e1ff",
            "6c86ab33fcaf4f8daf21e4f5e53d2f1f",
            "f214a7bfd99c415aa74614fcc62b87fd",
            "83993188f3ef422dbe04b580a38297e5",
            "5a5121b5285742cb9d254493fab35233",
            "dd0da857fa2b4eb6916764e10239642b",
            "faaa5163e33743418a0711af12a40951",
            "1fb8ebdafbed419c87d4b4a1fe1ab7ba",
            "6a87442948f644c894c4502f29c91e96",
            "1cd94ec6b6034578bbef09e0f7921f6e",
            "c16ce12516544536807017369067e1d3",
            "51455094a8d14868ad28e9a238951074",
            "d362804d450a437ebb58c79209ce55d2",
            "4cae0939344642448206d9b1fd8c92fc",
            "db964ff65cbb41da8727d10e031cddad",
            "54a991f97a9046a0be675b5edcf1f5db",
            "291bf5c1d9ca4369b8438d5d945978b9",
            "8d0b6f6d8d51413bb7645e154698d8b2",
            "c4ac98306d91473cb043f52f7b744a24"
          ]
        },
        "outputId": "662dd456-a50b-438c-d637-3a63a2f50315",
        "id": "b6mpsUPhDhzu"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "***** freeze embeddings *****\n",
            "***** freeze encoder *****\n",
            "trainable params: 201564160 || all params: 610879488 || trainable%: 32.99573221224282\n",
            "MBartForConditionalGeneration(\n",
            "  (model): MBartModel(\n",
            "    (shared): MBartScaledWordEmbedding(250054, 1024, padding_idx=1)\n",
            "    (encoder): MBartEncoder(\n",
            "      (embed_tokens): MBartScaledWordEmbedding(250054, 1024, padding_idx=1)\n",
            "      (embed_positions): MBartLearnedPositionalEmbedding(1026, 1024)\n",
            "      (layers): ModuleList(\n",
            "        (0-11): 12 x MBartEncoderLayer(\n",
            "          (self_attn): MBartAttention(\n",
            "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          )\n",
            "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "          (activation_fn): ReLU()\n",
            "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "      )\n",
            "      (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "    )\n",
            "    (decoder): MBartDecoder(\n",
            "      (embed_tokens): MBartScaledWordEmbedding(250054, 1024, padding_idx=1)\n",
            "      (embed_positions): MBartLearnedPositionalEmbedding(1026, 1024)\n",
            "      (layers): ModuleList(\n",
            "        (0-11): 12 x MBartDecoderLayer(\n",
            "          (self_attn): MBartAttention(\n",
            "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          )\n",
            "          (activation_fn): ReLU()\n",
            "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "          (encoder_attn): MBartAttention(\n",
            "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          )\n",
            "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "      )\n",
            "      (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "    )\n",
            "  )\n",
            "  (lm_head): Linear(in_features=1024, out_features=250054, bias=False)\n",
            ")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2109429943.py:114: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
            "  metric = load_metric('sacrebleu', trust_remote_code=True)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/5000 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1c337fcc0f344653bfcf1184e819e784"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7997d8194f51436b8b66cb1ed90991cb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6a87442948f644c894c4502f29c91e96"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='416' max='416' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [416/416 32:23, Epoch 1/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Bleu</th>\n",
              "      <th>Gen Len</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.885356</td>\n",
              "      <td>41.288600</td>\n",
              "      <td>32.465000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.856925</td>\n",
              "      <td>42.677100</td>\n",
              "      <td>31.655000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/transformers/generation/utils.py:1375: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
            "  warnings.warn(\n",
            "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
            "Non-default generation parameters: {'max_length': 200, 'early_stopping': True, 'num_beams': 5, 'forced_bos_token_id': 250003, 'forced_eos_token_id': 2}\n",
            "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
            "Non-default generation parameters: {'max_length': 200, 'early_stopping': True, 'num_beams': 5, 'forced_bos_token_id': 250003, 'forced_eos_token_id': 2}\n",
            "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
            "Non-default generation parameters: {'max_length': 200, 'early_stopping': True, 'num_beams': 5, 'forced_bos_token_id': 250003, 'forced_eos_token_id': 2}\n",
            "There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.decoder.embed_tokens.weight', 'lm_head.weight'].\n",
            "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
            "Non-default generation parameters: {'max_length': 200, 'early_stopping': True, 'num_beams': 5, 'forced_bos_token_id': 250003, 'forced_eos_token_id': 2}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#use models\n",
        "#import models and tokenizer\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "from datasets import load_from_disk\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"models/mbart50-full-finetune-1epoch-1e4\") #path of your directory with the finetuned\n",
        "\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\"models/mbart50-full-finetune-1epoch-1e4\") #gpu!!!\n",
        "\n",
        "#upload source file and read\n",
        "import codecs\n",
        "#file_name = \"Vienna_Environmental.en-de.valid.en\"\n",
        "mt_output = []\n",
        "output = open('mtouput_fullFT', 'w', encoding='utf-8')\n",
        "data = load_from_disk(\"data_save_test\")\n",
        "print(data)\n",
        "data['test']\n",
        "#with codecs.open(file_name, 'r', 'utf-8') as src:\n",
        "for line in data['test']: #trasn src and trg translation src and trg\n",
        "    src = line['translation']['en_XX']\n",
        "    trg = line['translation']['ru_RU']\n",
        "    #line = line.strip()\n",
        "    encoded = tokenizer(src, return_tensors=\"pt\")              #gpu?? tokenize\n",
        "    generated_tokens = model.generate(**encoded,\n",
        "                                      forced_bos_token_id=tokenizer.lang_code_to_id['ru_RU']) #add beam search 4\n",
        "    translation = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True) #de tokenize\n",
        "    print(translation[0], file=output)\n",
        "    print(translation[0])"
      ],
      "metadata": {
        "id": "oErXcPj-FGsL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}